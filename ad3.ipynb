{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43cde26b",
   "metadata": {},
   "source": [
    "**AD3**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170397ae",
   "metadata": {},
   "source": [
    "Esta es la **actividad dirigida 3**, que consiste en hacer un ejercicio de programación literaria aprovechando el código que hemos usado en programación con **Python** y donde realizaremos *web scraping*. \n",
    "A continuación el código fuente: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0947486b",
   "metadata": {},
   "source": [
    "## Código fuente ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468103a4",
   "metadata": {},
   "source": [
    "## Programación literaria ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53323273",
   "metadata": {},
   "source": [
    "### Módulos internos del sistema ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca69ad29",
   "metadata": {},
   "source": [
    "- [Time](https://docs.python.org/es/3/library/time.html), este módulo proporciona varias funciones relacionadas con el tiempo.\n",
    "- [Csv](https://docs.python.org/3/library/csv.html), es el formato de importación y exportación más común para hojas de cálculo y bases de datos.\n",
    "- [Re](https://docs.python.org/3/library/re.html), este módulo proporciona operaciones de coincidencia de expresiones regulares similares a las que se encuentran en Perl.\n",
    "- [Os](https://docs.python.org/3/library/os.html), este módulo proporciona una forma portátil de usar la funcionalidad dependiente del sistema operativo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0bd62f",
   "metadata": {},
   "source": [
    "### Librerías externas ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754676ad",
   "metadata": {},
   "source": [
    "- [Requests](https://requests.readthedocs.io/en/latest/): Esta es una librería de Python para trabajar con HTTP.\n",
    "- [bs4](https://pypi.org/project/beautifulsoup4/): Librería que permite recopilar información de páginas web.\n",
    "- [pandas](https://pypi.org/project/pandas/): Paquete de Python que ofrece estructuras de datos.\n",
    "- [termcolor](https://pypi.org/project/termcolor/): Librería que imprime el texto con color."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4271e36e",
   "metadata": {},
   "source": [
    "### Instalamos librerías ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa78128",
   "metadata": {},
   "source": [
    "Las librerías que vienen con Python no se deben instalar, las demás deben sí."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e6b6b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\linda\\anaconda3\\lib\\site-packages (2.27.1)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: bs4 in c:\\users\\linda\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\linda\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "\n",
      "Requirement already satisfied: termcolor in c:\\users\\linda\\anaconda3\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\linda\\anaconda3\\lib\\site-packages (from requests) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\linda\\anaconda3\\lib\\site-packages (from requests) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\linda\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\linda\\anaconda3\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\linda\\anaconda3\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\linda\\anaconda3\\lib\\site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\linda\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\linda\\anaconda3\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\linda\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\linda\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.1)\n"
     ]
    }
   ],
   "source": [
    "pip install requests bs4 pandas termcolor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bf644c",
   "metadata": {},
   "source": [
    "### Importamos librerías ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c934a8",
   "metadata": {},
   "source": [
    "Importación de librerías: \n",
    "- Librería <code>request</code> \n",
    "- Librería <code>bs4</code>\n",
    "- Librería <code>pandas</code>\n",
    "- Librería <code>termcolor</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e1e4aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e18e79",
   "metadata": {},
   "source": [
    "## Objetos variables ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b694dd",
   "metadata": {},
   "source": [
    "A continuación objetos o variables:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee88afec",
   "metadata": {},
   "source": [
    "#### Resultados ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc5c4fd",
   "metadata": {},
   "source": [
    "Creamos un objeto vacío denominado 'resultados()' en el que se añadirán los resultados del *scrapping*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed76ac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af986bfe",
   "metadata": {},
   "source": [
    "Si pasamos la función type() podemos observar que este objeto es de tipo \"lista\" de Python. Este tipo de elemento es básico y fundamental para organizar información/datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cdc45ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(resultados)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70950418",
   "metadata": {},
   "source": [
    "### Solicitud de acceso a datos en un sitio en línea ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9640471",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d3443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "from termcolor import colored\n",
    "\n",
    "resultados = []\n",
    "\n",
    "req = requests.get(\"https://resultados.elpais.com\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup = BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "tags = soup.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req2 = requests.get(\"https://elpais.com/internacional\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup2 = BeautifulSoup(req2.text, 'html.parser')\n",
    "\n",
    "tags = soup2.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req3 = requests.get(\"https://elpais.com/opinion\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup3 = BeautifulSoup(req3.text, 'html.parser')\n",
    "\n",
    "tags = soup3.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req4 = requests.get(\"https://elpais.com/espana/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup4 = BeautifulSoup(req4.text, 'html.parser')\n",
    "\n",
    "tags = soup4.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req5 = requests.get(\"https://elpais.com/economia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup5 = BeautifulSoup(req5.text, 'html.parser')\n",
    "\n",
    "tags = soup5.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req6 = requests.get(\"https://elpais.com/sociedad/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup6 = BeautifulSoup(req6.text, 'html.parser')\n",
    "\n",
    "tags = soup6.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req7 = requests.get(\"https://elpais.com/educacion/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup7 = BeautifulSoup(req7.text, 'html.parser')\n",
    "\n",
    "tags = soup7.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req8 = requests.get(\"https://elpais.com/clima-y-medio-ambiente/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup8 = BeautifulSoup(req8.text, 'html.parser')\n",
    "\n",
    "tags = soup8.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req9 = requests.get(\"https://elpais.com/ciencia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup9 = BeautifulSoup(req9.text, 'html.parser')\n",
    "\n",
    "tags = soup9.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req10 = requests.get(\"https://elpais.com/cultura/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup10 = BeautifulSoup(req10.text, 'html.parser')\n",
    "\n",
    "tags = soup10.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req11 = requests.get(\"https://elpais.com/babelia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup11 = BeautifulSoup(req11.text, 'html.parser')\n",
    "\n",
    "tags = soup11.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req12 = requests.get(\"https://elpais.com/deportes/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup12 = BeautifulSoup(req12.text, 'html.parser')\n",
    "\n",
    "tags = soup12.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req13 = requests.get(\"https://elpais.com/tecnologia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup13 = BeautifulSoup(req13.text, 'html.parser')\n",
    "\n",
    "tags = soup13.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req14 = requests.get(\"https://elpais.com/tecnologia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup14 = BeautifulSoup(req14.text, 'html.parser')\n",
    "\n",
    "tags = soup14.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req15 = requests.get(\"https://elpais.com/gente/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup15 = BeautifulSoup(req15.text, 'html.parser')\n",
    "\n",
    "tags = soup15.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req16 = requests.get(\"https://elpais.com/television/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup16 = BeautifulSoup(req16.text, 'html.parser')\n",
    "\n",
    "tags = soup16.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req17 = requests.get(\"https://elpais.com/eps/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup17 = BeautifulSoup(req17.text, 'html.parser')\n",
    "\n",
    "tags = soup17.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "\n",
    "os.system(\"clear\")\n",
    "\n",
    "print(colored(\"A continuación se muestran los titulares de las páginas principales del diario El País que contienen las siguientes palabras:\", 'blue', attrs=['bold']))\n",
    "print(colored(\"Feminismo\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"feminismo\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Igualdad\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"igualdad\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Mujeres\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"mujeres\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Mujer\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"mujer\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Brecha salarial\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"brecha salarial\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Machismo\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"machismo\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Violencia\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"violencia\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Maltrato\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"maltrato\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Homicidio\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"homicidio\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Género\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"género\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Asesinato\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"asesinato\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Sexo\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"sexo\" in s]\n",
    "print(\"\\n\".join(str_match))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
